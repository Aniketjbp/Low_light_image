{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT0LPkKP62uB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1DdGIJ4PZPlF2ikl8mNM9V-PdVxVLbQi6\n",
        "!unzip -q lol_dataset.zip"
      ],
      "metadata": {
        "id": "LeGfwsIEDf2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "\n",
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 4\n",
        "MAX_TRAIN_IMAGES = 300\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image.set_shape([None, None, 3])\n",
        "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_crop(low_image, enhanced_image):\n",
        "    low_image_shape = tf.shape(low_image)[:2]\n",
        "    low_w = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    low_h = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
        "    )\n",
        "    enhanced_w = low_w\n",
        "    enhanced_h = low_h\n",
        "    low_image_cropped = low_image[\n",
        "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
        "    ]\n",
        "    enhanced_image_cropped = enhanced_image[\n",
        "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
        "    ]\n",
        "    return low_image_cropped, enhanced_image_cropped\n",
        "\n",
        "\n",
        "def load_data(low_light_image_path, enhanced_image_path):\n",
        "    low_light_image = read_image(low_light_image_path)\n",
        "    enhanced_image = read_image(enhanced_image_path)\n",
        "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
        "    return low_light_image, enhanced_image\n",
        "\n",
        "\n",
        "def get_dataset(low_light_images, enhanced_images):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n",
        "\n",
        "val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n",
        "\n",
        "test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
        "test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n",
        "\n",
        "\n",
        "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
        "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
        "\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "DD-3p2HqDq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selective_kernel_feature_fusion(\n",
        "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
        "):\n",
        "    channels = list(multi_scale_feature_1.shape)[-1]\n",
        "    combined_feature = layers.Add()(\n",
        "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
        "    )\n",
        "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
        "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
        "    compact_feature_representation = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(channel_wise_statistics)\n",
        "    feature_descriptor_1 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_2 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_descriptor_3 = layers.Conv2D(\n",
        "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
        "    )(compact_feature_representation)\n",
        "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
        "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
        "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
        "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
        "    return aggregated_feature\n"
      ],
      "metadata": {
        "id": "kHo8pBo6DyRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_attention_block(input_tensor):\n",
        "    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n",
        "    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n",
        "    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n",
        "    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n",
        "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
        "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n",
        "    feature_map = tf.nn.sigmoid(feature_map)\n",
        "    return input_tensor * feature_map\n",
        "\n",
        "\n",
        "def channel_attention_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
        "    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
        "    )(feature_descriptor)\n",
        "    feature_activations = layers.Conv2D(\n",
        "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
        "    )(feature_activations)\n",
        "    return input_tensor * feature_activations\n",
        "\n",
        "\n",
        "def dual_attention_unit_block(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    feature_map = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(input_tensor)\n",
        "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
        "        feature_map\n",
        "    )\n",
        "    channel_attention = channel_attention_block(feature_map)\n",
        "    spatial_attention = spatial_attention_block(feature_map)\n",
        "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
        "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
        "    return layers.Add()([input_tensor, concatenation])\n"
      ],
      "metadata": {
        "id": "ntaoojOJD2uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive Residual Modules\n",
        "\n",
        "def down_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.MaxPooling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])\n",
        "\n",
        "\n",
        "def up_sampling_module(input_tensor):\n",
        "    channels = list(input_tensor.shape)[-1]\n",
        "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
        "        input_tensor\n",
        "    )\n",
        "    main_branch = layers.Conv2D(\n",
        "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
        "    )(main_branch)\n",
        "    main_branch = layers.UpSampling2D()(main_branch)\n",
        "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
        "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
        "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
        "    return layers.Add()([skip_branch, main_branch])\n",
        "\n",
        "\n",
        "# MRB Block\n",
        "def multi_scale_residual_block(input_tensor, channels):\n",
        "    # features\n",
        "    level1 = input_tensor\n",
        "    level2 = down_sampling_module(input_tensor)\n",
        "    level3 = down_sampling_module(level2)\n",
        "    # DAU\n",
        "    level1_dau = dual_attention_unit_block(level1)\n",
        "    level2_dau = dual_attention_unit_block(level2)\n",
        "    level3_dau = dual_attention_unit_block(level3)\n",
        "    # SKFF\n",
        "    level1_skff = selective_kernel_feature_fusion(\n",
        "        level1_dau,\n",
        "        up_sampling_module(level2_dau),\n",
        "        up_sampling_module(up_sampling_module(level3_dau)),\n",
        "    )\n",
        "    level2_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n",
        "    )\n",
        "    level3_skff = selective_kernel_feature_fusion(\n",
        "        down_sampling_module(down_sampling_module(level1_dau)),\n",
        "        down_sampling_module(level2_dau),\n",
        "        level3_dau,\n",
        "    )\n",
        "    # DAU 2\n",
        "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
        "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
        "    level3_dau_2 = up_sampling_module(\n",
        "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
        "    )\n",
        "    # SKFF 2\n",
        "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level3_dau_2, level3_dau_2)\n",
        "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
        "    return layers.Add()([input_tensor, conv])\n"
      ],
      "metadata": {
        "id": "3gnpxdlcD6T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
        "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_mrb):\n",
        "        conv1 = multi_scale_residual_block(conv1, channels)\n",
        "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
        "    return layers.Add()([conv2, input_tensor])\n",
        "\n",
        "\n",
        "def mirnet_model(num_rrg, num_mrb, channels):\n",
        "    input_tensor = keras.Input(shape=[None, None, 3])\n",
        "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
        "    for _ in range(num_rrg):\n",
        "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
        "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
        "    output_tensor = layers.Add()([input_tensor, conv])\n",
        "    return keras.Model(input_tensor, output_tensor)\n",
        "\n",
        "\n",
        "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"
      ],
      "metadata": {
        "id": "fmU4hrAED-Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def charbonnier_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
        "\n",
        "\n",
        "def peak_signal_noise_ratio(y_true, y_pred):\n",
        "    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_peak_signal_noise_ratio\",\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            min_delta=1e-7,\n",
        "            mode=\"max\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
        "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8FmRaDAvEDCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a model using the SavedModel format\n",
        "model.save('best_model',save_traces=True)"
      ],
      "metadata": {
        "id": "1Uq17VQKELh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(images, titles, figure_size=(12, 12)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _ = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def infer(original_image):\n",
        "    image = keras.preprocessing.image.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output = model.predict(image)\n",
        "    output_image = output[0] * 255.0\n",
        "    output_image = output_image.clip(0, 255)\n",
        "    output_image = output_image.reshape(\n",
        "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
        "    )\n",
        "    output_image = Image.fromarray(np.uint8(output_image))\n",
        "    original_image = Image.fromarray(np.uint8(original_image))\n",
        "    return output_image\n"
      ],
      "metadata": {
        "id": "Kjbew1BEd9eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for low_light_image in random.sample(test_low_light_images, 6):\n",
        "    original_image = Image.open(low_light_image)\n",
        "    enhanced_image = infer(original_image)\n",
        "    plot_results(\n",
        "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
        "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ],
      "metadata": {
        "id": "YMgKgQ0QcUgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy('','destination')"
      ],
      "metadata": {
        "id": "-VX_Phse_usI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "Ch3w7fxcAHZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_1=tf.io.read_file('/content/test3.jpeg')\n",
        "#img_1=tf.image.decode_png(img_1, channels=3)\n",
        "#print(img_1.shape)\n"
      ],
      "metadata": {
        "id": "RNLxAUAjdGDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img_1=tf.image.resize(img_1, [128,128], antialias=True)\n",
        "#img_1.set_shape([None, None, 3])"
      ],
      "metadata": {
        "id": "O8PSFvxkgfPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_1=Image.open('/content/test4.jpeg')\n",
        "enhanced_image = infer(img_1)\n",
        "plot_results(\n",
        "        [img_1, ImageOps.autocontrast(img_1), enhanced_image],\n",
        "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ],
      "metadata": {
        "id": "E_vbP2bveTvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEUusOcSoRTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}